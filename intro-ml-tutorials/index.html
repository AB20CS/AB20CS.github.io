<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Introduction to Machine Learning Tutorials | Anindro Bhattacharya</title> <meta name="author" content="Anindro Bhattacharya"> <meta name="description" content="A portfolio of my experiences "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ab20cs.github.io/intro-ml-tutorials/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Anindro </span>Bhattacharya</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Introduction to Machine Learning Tutorials</h1> <p class="post-description"></p> </header> <article> <p>On this page, you can find primers on introductory machine learning algorithms.</p> <h1 id="linear-regression">Linear Regression</h1> <p>In linear regression, our goal is to simplify our data into a line that can capture the overall trend.</p> <p>In other words, we would like to fit the following model onto our data:</p> \[y = wx + b\] <p>where \(w\) is the weight and \(b\) is the bias.</p> <p><em>To Determine:</em> Estimate \(w\) and \(b\) from \(N\) training pairs \(\{(x_i, y_i\}_{i=1}^N\).</p> <p><em>Goal:</em> With determined \(w\) and \(b\), we would like to calculate \(y\) for some new inputs \(x\).</p> <p>Let \(e_i\) be the vertical distance from the line to the training point.</p> \[\begin{equation} e_i = y_i - (wx_i + b) \end{equation}\] <p><strong>Objective Function:</strong> We define our objective function \(E(w,b)\) as follows: \(\begin{equation} \begin{split} E(w,b) &amp;= \sum_{i=1}^N e_i^2 \\ &amp;= \sum_{i=1}^N y_i - (wx_i + b)^2 \end{split} \end{equation}\)</p> <p>We would like to minimize \(E(w,b)\) in linear regression (i.e., we want the parameters when \(\nabla E(w,b)=0\)).</p> <p>This means that \(\frac{\partial E}{\partial w} = 0\) and \(\frac{\partial E}{\partial b} = 0\).</p> <p>Let us find \(b^*\) and \(w^*\) - the values of \(b\) and \(w\), respectively, that minimize \(E(w,b)\).</p> <p><strong>Find \(b^*\):</strong></p> <p>Let us first find an expression for \(\frac{\partial E}{\partial b}\).</p> \[\begin{equation} \begin{split} \frac{\partial E}{\partial b} &amp;= -2 \left( \sum_{i=1}^N y_i - (wx_i + b) \right) \\ &amp;= -2 \left( \sum_{i=1}^N y_i - wx_i - b \right) \\ &amp;= -2 \left( \sum_{i=1}^N y_i - \sum_{i=1}^N wx_i - \sum_{i=1}^N b \right) \\ &amp;= -2 \left( \sum_{i=1}^N y_i - w \sum_{i=1}^N x_i - \sum_{i=1}^N b \right) \\ &amp;= -2 \left( \sum_{i=1}^N y_i - w \sum_{i=1}^N x_i - Nb \right) \\ \end{split} \end{equation}\] <p>Setting \(\frac{\partial E}{\partial b} = 0\), we get \(\begin{equation} \begin{split} b^* &amp;= \frac{\sum_{i=1}^N y_i}{N} - \frac{w \sum_{i=1}^N x_i}{N} \\ &amp;= \hat{y} - w \hat{x} \end{split} \end{equation}\)</p> <p>where \(\hat{x}\) and \(\hat{y}\) are the averages of all \(x\) and \(y\) values, respectively.</p> <p><strong>Find \(w^*\):</strong></p> <p>Now that we have an expression for \(b^*\), we can re-write \(E(w,b)\) as \(\begin{equation} \begin{split} E(w,b) = \sum_{i=1}^N ((y_i - \hat{y}) - w(x_i - \hat{x}))^2 \end{split} \end{equation}\)</p> <p>Solving for \(\frac{\partial E}{\partial w}\), we obtain the following</p> \[\begin{equation} \begin{split} \frac{\partial E}{\partial w} &amp;= -2 \left( \sum_{i=1}^N ((y_i - \hat{y}) - w(x_i - \hat{x})) \right) \\ &amp;= -2 \left( \sum_{i=1}^N ((y_i - \hat{y})(x_i - \hat{x})) - w \sum_{i=1}^N (x_i - \hat{x})^2 \right) \\ \end{split} \end{equation}\] <p>Setting \(\frac{\partial E}{\partial w}=0\), we get \(\begin{equation} \begin{split} w^* &amp;= \frac{\sum_{i=1}^N ((y_i - \hat{y})(x_i - \hat{x}))}{\sum_{i=1}^N (x_i - \hat{x})^2} \end{split} \end{equation}\)</p> <p>In this way, we can derive the optimal parameters for linear regression using least-squares and fit a line on our dataset.</p> <h1 id="clustering-and-the-k-means-algorithm">Clustering and the K-Means Algorithm</h1> <p>Clustering is an <strong>unsupervised</strong> learning problem in which we seek to group our data into clusters based on similarity.</p> <h2 id="k-means-algorithm">K-Means Algorithm</h2> <p><em>Adapted from https://towardsdatascience.com/k-means-clustering-introduction-to-machine-learning-algorithms-c96bf0d5d57a</em></p> <ol> <li>Choose the number of clusters, \(K\), and obtain the data points.</li> <li>Initialize the centroids \(c_1, c_2, ..., c_K\). This can be done in various ways (e.g., random, \(K\)-medoids, multiple restarts).</li> <li>Repeat until convergence or fixed number of iterations: <ul> <li>for each data point \(x_i\): <code class="language-plaintext highlighter-rouge"># update cluster labels</code> <ul> <li>find the nearest centroid \(c_j\) to \(x_i\)</li> <li>assign point \(x_i\) to the cluster represented by the centroid \(c_j\)</li> </ul> </li> <li>for each cluster \(j = 1, \dots, K\):<code class="language-plaintext highlighter-rouge"># update centroid locations</code> <ul> <li>\(c_j\) = mean of all points assigned to cluster \(j\)</li> </ul> </li> </ul> </li> </ol> <h2 id="elbow-method">“Elbow” Method</h2> <p><strong>Problem:</strong> What if the number of clusters was not obvious? How would we initialize the number of centroids? <strong>Solution:</strong> We can calculate the <strong>Within-Cluster Sum of Squares (WCSS)</strong> for every number of clusters we would like to examine. Then, we could plot these WCSS values. The number of clusters at which the plot starts to plateau (looks like an “elbow”) is the optimal number of clusters according to this metric. We define WCSS as follows:</p> \[\begin{equation} WCSS = \sum_{C_k}^{C_n} \sum_{d_i \text{ in } C_k}^{d_m} \text{distance}(d_i, C_k)^2 \end{equation}\] <p>where \(C_k\) is the \(i^{\text{th}}\) centroid and \(d_i\) is \(i^{\text{th}}\) data point in the centroid \(C_k\).</p> <p>In the <code class="language-plaintext highlighter-rouge">KMeans</code> class within Scikit-Learn, the attribute <code class="language-plaintext highlighter-rouge">inertia_</code> stores the WCSS.</p> <h1 id="cross-validation">Cross-Validation</h1> <p>Cross-validation is an empirical approach for evaluating a model’s performance by dividing the dataset into several parts, using different parts for testing and training in successive iterations.</p> <p><strong>Cross-validation can be used for:</strong></p> <ul> <li>Mitigating the risk of overfitting</li> <li>Comparing different models and selecting the one performing best on average</li> <li>Determining appropriate hyperparameter values</li> </ul> <p><strong>Disadvantages:</strong></p> <ul> <li>Can be computationally expensive</li> <li>Might not be appropriate for certain data sets</li> </ul> <p><strong>\(N\)-Fold Cross-Validation Algorithm</strong></p> <ol> <li>Split dataset into \(N\) equal partitions</li> <li>Use one fold for testing and the other \(N-1\) folds for training</li> <li>Calculate performance on the test set</li> <li>Repeat Steps 2 and 3 \(N\) times</li> <li>Use average accuracy as an estimate of model’s performance</li> </ol> <p>When \(N=M-1\), where \(M\) is the number of data points, this approach is called <strong>leave-one-out cross-validation</strong>.</p> </article> </div> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Anindro Bhattacharya. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>